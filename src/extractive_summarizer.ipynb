{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization in Python\n",
    "\n",
    "## Motivation: \n",
    "The length of textual data is increasing and people have less time. Often the newspaper articles run into a long text of, say 1000 -1200 words. As wearable devices leap to prominence (Google Glass, Apple Watch, to name a few), content must adapt to the limited screen space available on these devices.\n",
    "The task of generating intelligent and accurate summaries for long pieces of text has become a popular research as well as industry problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach: \n",
    "Extractive text summarization is all about finding the more important sentences from a document as a summary of that document.\n",
    "Our approach is using the TextRank algorithm to find these 'important' sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy library helps in working with arrays: array creation and manipulation\n",
    "# this implementation uses array for storing the matrices generated as 2-D arrays\n",
    "# PyPDF2 is a library used for reading the PDF files\n",
    "# docx2txt is the library used for reading Word documents \n",
    "# sys library has been used for printing the size of data structures used in the program\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import docx2txt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib is a library that is used to visualize the data by drawing graphs of matrix inputs\n",
    "# we will use it for drawing the matrices generated later in the program \n",
    "# %matplotlib inline is a command used to show the graphs in the jupyter notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# networkx library helps in working with graphs ...\n",
    "# and later performing the PageRank algorithm ...\n",
    "# which is the crux of this implementation to find ...\n",
    "# the importance of each sentence using their 'rank' as a metric ...\n",
    "# rank, the output of the method textrank, is a measure of importance of sentences\n",
    "# this library has been used in the cell no. ()\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PunktSentenceTokenizer library is being imported from the file punkt.py contained in package nltk.tokenize \n",
    "# this is used to tokenize the document into sentences\n",
    "\n",
    "# Tokenization: Tokenization is the process of demarcating and possibly classifying.. \n",
    "# sections of a string of input characters. \n",
    "# The resulting tokens are then passed on to some other form of processing. \n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer and CountVectorizer libraries are being imported\n",
    "\n",
    "# CountVectorizer: In this implementation, a CountVectorizer object is being created that ..\n",
    "# will be used for creating the document-term matrix\n",
    "\n",
    "# tFidTransformer: In this implementation,TfidfTransformer is used for executing the method fit_transform()... \n",
    "# which provides the output as a document-term matrix normalized (value 0-1) according to the TF-IDF\n",
    "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
    "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
    "# Corpus: set of all sentences\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Function to read the document from user\n",
    "Supported formats: .txt, .pdf \n",
    "\n",
    "Input: Takes the name of the file as input. \n",
    "\n",
    "Output: Returns a string output containing the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to show an example of how the method is working\n",
    "# first let's take the document as an input\n",
    "def readDoc():\n",
    "    name = input('Please input a file name: ') \n",
    "    print('You have asked for the document {}'.format(name))\n",
    "\n",
    "    # now read the type of document\n",
    "    if name.lower().endswith('.txt'):\n",
    "        choice = 1\n",
    "    elif name.lower().endswith('.pdf'):\n",
    "        choice = 2\n",
    "    else:\n",
    "        choice = 3\n",
    "        # print(name)\n",
    "    print(choice)\n",
    "    # Case 1: if it is a .txt file\n",
    "        \n",
    "    if choice == 1:\n",
    "        f = open(name, 'r')\n",
    "        document = f.read()\n",
    "        f.close()\n",
    "            \n",
    "    # Case 2: if it is a .pdf file\n",
    "    elif choice == 2:\n",
    "        pdfFileObj = open(name, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pageObj = pdfReader.getPage(0)\n",
    "        document = pageObj.extractText()\n",
    "        pdfFileObj.close()\n",
    "    \n",
    "    # Case 3: none of the format\n",
    "    else:\n",
    "        print('Failed to load a valid file')\n",
    "        print('Returning an empty string')\n",
    "        document = ''\n",
    "    \n",
    "    print(type(document))\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to tokenize the document\n",
    "Input: String of text document\n",
    "\n",
    "Output: A list containing sentences as its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function used for tokenizing the sentences\n",
    "# tokenization of a sentence: '''provided in cell() above'''\n",
    "\n",
    "def tokenize(document):\n",
    "    # We are tokenizing using the PunktSentenceTokenizer\n",
    "    # we call an instance of this class as sentence_tokenizer\n",
    "    doc_tokenizer = PunktSentenceTokenizer()\n",
    "    \n",
    "    # tokenize() method: takes our document as input and returns a list of all the sentences in the document\n",
    "    \n",
    "    # sentences is a list containing each sentence of the document as an element\n",
    "    sentences_list = doc_tokenizer.tokenize(document)\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Read the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a file name: story2.txt\n",
      "You have asked for the document story2.txt\n",
      "1\n",
      "<class 'str'>\n",
      "The length of the file is: 612\n"
     ]
    }
   ],
   "source": [
    "# reading a file and \n",
    "# printing the size of the file\n",
    "document = readDoc()\n",
    "print('The length of the file is:', end=' ')\n",
    "print(len(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate a list of sentences in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the list in Bytes is: 120\n",
      "The size of the item 0 in Bytes is: 249\n"
     ]
    }
   ],
   "source": [
    "# we want to tokenize the document for further processing\n",
    "# tokenizing the sentence means that we are creating a list of all the sentences of the document.\n",
    "# Need of tokenizing the document: Initially the document is in just a string format.\n",
    "# if we want to process the document, we need to store it in a data structure.\n",
    "# Tokenization of document into words is also possible, but we will go with the tokenizing with the sentences\n",
    "# Since we want to choose the most relevant sentences, we need to generate tokens of sentences only\n",
    "sentences_list = tokenize(document)\n",
    "\n",
    "# let us print the size of memory used by the list sentences\n",
    "print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))\n",
    "\n",
    "# the size of one of the element of the list\n",
    "print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# let us see the data type of sentences_list\n",
    "# It will be list\n",
    "print(type(sentences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the list \"sentences\" is: 4\n"
     ]
    }
   ],
   "source": [
    "# let us analyse the elements of the sentences\n",
    "# len() method applies on the list and provides the number of elements in the list\n",
    "print('The size of the list \"sentences\" is: {}'.format(len(sentences_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate term-document matrix (TD matrix) of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "# fit_transform method of CountVectorizer() class \n",
    "# Learn the vocabulary dictionary and return term-document matrix. \n",
    "# I/p: An iterable which yields either str, unicode or file objects.\n",
    "# O/p: The term-document matrix named cv_matrix\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(sentences_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what does CountVectorizer.fit_transform() do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result demo array is [[0 1 1 1 1 1 0 1]\n",
      " [1 0 0 1 0 0 1 0]]\n",
      "Feature list: ['am', 'are', 'ashish', 'bad', 'good', 'is', 'not', 'you']\n"
     ]
    }
   ],
   "source": [
    "# a demo of what CountVectorizer().fit_transform(text) does\n",
    "cv_demo = CountVectorizer() # a demo object of class CountVectorizer\n",
    "\n",
    "# I have repeated the words to make a non-ambiguous array of the document text matrix \n",
    "\n",
    "text_demo = [\"Ashish is good, you are bad\", \"I am not bad\"] \n",
    "res_demo = cv_demo.fit_transform(text_demo)\n",
    "print('Result demo array is {}'.format(res_demo.toarray()))\n",
    "\n",
    "# Result is 2-d matrix containing document text matrix\n",
    "# Notice that in the second row, there is 2.\n",
    "# also, bad is repeated twice in that sentence.\n",
    "# so we can infer that 2 is corresponding to the word 'bad'\n",
    "print('Feature list: {}'.format(cv_demo.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of bow matrix <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of the matrix <bound method spmatrix.get_shape of <4x70 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 85 stored elements in Compressed Sparse Row format>>\n",
      "Size of the matrix is: 56\n",
      "['212th', '500', 'added', 'an', 'area', 'around', 'away', 'bastar', 'battalion', 'capital', 'central', 'chhattisgarh', 'combing', 'crpf', 'cum', 'deceased', 'dense', 'deputy', 'district', 'domination', 'force', 'forces', 'forest', 'from', 'general', 'gunfight', 'have', 'he', 'hit', 'in', 'inspector', 'insurgency', 'killed', 'kistaram', 'km', 'last', 'launched', 'located', 'madhya', 'maoists', 'native', 'night', 'of', 'officer', 'on', 'operation', 'out', 'place', 'police', 'pradesh', 'pti', 'raipur', 'range', 'reserve', 'said', 'saturday', 'search', 'security', 'south', 'state', 'station', 'sukma', 'sundarraj', 'team', 'the', 'told', 'took', 'was', 'when', 'with']\n",
      "[[0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 3 0 1 1 0 0 0\n",
      "  0 0 0 1 0 0 2 2 1 0 0 0 2 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 3 0 0 1 0 1]\n",
      " [1 0 0 0 2 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
      "  0 0 0 0 0 1 2 0 1 1 1 1 2 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      " [0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0\n",
      "  1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 2 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing the cv_matrix type\n",
    "# and how it is being stored in memory?\n",
    "# it is stored in the compressed row format\n",
    "# compressed row format: \n",
    "print('The data type of bow matrix {}'.format(type(cv_matrix)))\n",
    "print('Shape of the matrix {}'.format(cv_matrix.get_shape))\n",
    "print('Size of the matrix is: {}'.format(sys.getsizeof(cv_matrix)))\n",
    "print(cv.get_feature_names())\n",
    "print(cv_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.17539606 0.         0.\n",
      "  0.         0.         0.         0.         0.17539606 0.17539606\n",
      "  0.         0.13828427 0.         0.         0.17539606 0.\n",
      "  0.17539606 0.         0.17539606 0.         0.17539606 0.\n",
      "  0.         0.13828427 0.         0.         0.17539606 0.3358591\n",
      "  0.         0.17539606 0.17539606 0.         0.         0.\n",
      "  0.         0.         0.         0.17539606 0.         0.\n",
      "  0.22390607 0.35079212 0.13828427 0.         0.         0.\n",
      "  0.27656854 0.         0.         0.         0.         0.17539606\n",
      "  0.17539606 0.17539606 0.         0.         0.         0.\n",
      "  0.         0.17539606 0.         0.         0.27458682 0.\n",
      "  0.         0.11195303 0.         0.17539606]\n",
      " [0.17361558 0.         0.         0.         0.27376103 0.\n",
      "  0.         0.17361558 0.17361558 0.         0.         0.\n",
      "  0.         0.13688052 0.17361558 0.         0.         0.17361558\n",
      "  0.         0.17361558 0.         0.         0.         0.\n",
      "  0.17361558 0.13688052 0.         0.         0.         0.11081657\n",
      "  0.17361558 0.         0.         0.17361558 0.         0.17361558\n",
      "  0.         0.         0.         0.         0.         0.17361558\n",
      "  0.22163315 0.         0.13688052 0.13688052 0.17361558 0.17361558\n",
      "  0.27376103 0.         0.17361558 0.         0.17361558 0.\n",
      "  0.         0.         0.17361558 0.         0.17361558 0.\n",
      "  0.17361558 0.         0.17361558 0.17361558 0.09059981 0.17361558\n",
      "  0.17361558 0.11081657 0.17361558 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.44336682 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.44336682 0.         0.44336682 0.\n",
      "  0.2829953  0.         0.         0.         0.         0.\n",
      "  0.         0.44336682 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2313672  0.\n",
      "  0.         0.2829953  0.         0.        ]\n",
      " [0.         0.23100252 0.23100252 0.         0.18212504 0.23100252\n",
      "  0.23100252 0.         0.         0.23100252 0.         0.\n",
      "  0.23100252 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23100252 0.         0.23100252\n",
      "  0.         0.         0.23100252 0.23100252 0.         0.14744592\n",
      "  0.         0.         0.         0.         0.23100252 0.\n",
      "  0.23100252 0.23100252 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18212504 0.         0.\n",
      "  0.         0.         0.         0.23100252 0.         0.\n",
      "  0.         0.         0.         0.23100252 0.         0.23100252\n",
      "  0.         0.         0.         0.         0.24109339 0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Tnormalized: document-term matrix normalized (value 0-1) according to the TF-IDF\n",
    "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
    "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
    "# Corpus: set of all sentences\n",
    "\n",
    "normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n",
    "print(normal_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _cs_matrix.toarray of <70x4 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 85 stored elements in Compressed Sparse Column format>>\n"
     ]
    }
   ],
   "source": [
    "print(normal_matrix.T.toarray)\n",
    "res_graph = normal_matrix * normal_matrix.T\n",
    "# plt.spy(res_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges 10\n",
      "Number of vertices 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory used by the graph in Bytes is: 56\n"
     ]
    }
   ],
   "source": [
    "# drawing a graph to proceed for the textrank algorithm\n",
    "# nx_graph is a graph developed using the networkx library\n",
    "# each node represents a sentence\n",
    "# an edge represents that they have words in common\n",
    "# the edge weight is the number of words that are common in both of the sentences(nodes)\n",
    "# nx.draw() method is used to draw the graph created\n",
    "\n",
    "nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n",
    "nx.draw_circular(nx_graph)\n",
    "print('Number of edges {}'.format(nx_graph.number_of_edges()))\n",
    "print('Number of vertices {}'.format(nx_graph.number_of_nodes()))\n",
    "plt.show()\n",
    "print('The memory used by the graph in Bytes is: {}'.format(sys.getsizeof(nx_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  note that the graph above is dense and therefor it resembles a circle\n",
    "# if a shorter document is taken, a beautiful circular graph can be seen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Getting the rank of every sentence using textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "The size used by the dictionary in Bytes is: 240\n",
      "0 0.265145355988123\n",
      "1 0.2587001796429536\n",
      "2 0.24013487976684236\n",
      "3 0.2360195846020809\n"
     ]
    }
   ],
   "source": [
    "# ranks is a dictionary with key=node(sentences) and value=textrank (the rank of each of the sentences)\n",
    "ranks = nx.pagerank(nx_graph)\n",
    "\n",
    "# analyse the data type of ranks\n",
    "print(type(ranks))\n",
    "print('The size used by the dictionary in Bytes is: {}'.format(sys.getsizeof(ranks)))\n",
    "\n",
    "# print the dictionary\n",
    "for i in ranks:\n",
    "    print(i, ranks[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Finding important sentences and generating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate method: returns an enumerate object\n",
    "# Use of list Comprehensions\n",
    "# O/p: sentence_array is the sorted(descending order w.r.t. score value) 2-d array of ranks[sentence] and sentence \n",
    "# For example, if there are two sentences: S1 (with a score of S1 = s1) and S2 with score s2, with s2>s1\n",
    "# then sentence_array is [[s2, S2], [s1, S1]]\n",
    "sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n",
    "sentence_array = np.asarray(sentence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as sentence_array is in descending order wrt score value\n",
    "# fmax is the largest score value(the score of first element)\n",
    "# fmin is the smallest score value(the score of last element)\n",
    "\n",
    "rank_max = float(sentence_array[0][0])\n",
    "rank_min = float(sentence_array[len(sentence_array) - 1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.265145355988123\n",
      "0.2360195846020809\n"
     ]
    }
   ],
   "source": [
    "# print the largest and smallest value of scores of the sentence\n",
    "print(rank_max)\n",
    "print(rank_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Normalization of the scores\n",
    "# so that it comes out in the range 0-1\n",
    "# fmax becomes 1\n",
    "# fmin becomes 0\n",
    "# store the normalized values in the list temp_array\n",
    "\n",
    "temp_array = []\n",
    "\n",
    "# if all sentences have equal ranks, means they are all the same\n",
    "# taking any sentence will give the summary, say the first sentence\n",
    "flag = 0\n",
    "if rank_max - rank_min == 0:\n",
    "    temp_array.append(0)\n",
    "    flag = 1\n",
    "\n",
    "# If the sentence has different ranks\n",
    "if flag != 1:\n",
    "    for i in range(0, len(sentence_array)):\n",
    "        temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n",
    "\n",
    "print(len(temp_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of threshold:\n",
    "# We take the mean value of normalized scores\n",
    "# any sentence with the normalized score 0.2 more than the mean value is considered to be \n",
    "threshold = (sum(temp_array) / len(temp_array)) + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the sentences that satiasfy the criteria of having a score above the threshold\n",
    "sentence_list = []\n",
    "if len(temp_array) > 1:\n",
    "    for i in range(0, len(temp_array)):\n",
    "        if temp_array[i] > threshold:\n",
    "                sentence_list.append(sentence_array[i][1])\n",
    "else:\n",
    "    sentence_list.append(sentence_array[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Writing the summary to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An officer of the Central Reserve Police Force (CRPF) officer was killed in a gunfight with the Maoists in a dense forest in the insurgency-hit Sukma district of Chhattisgarh, police said on Saturday. The gunfight took place last night in Kistaram police station area when a team of CRPF's 212th battalion was out on a search-cum-area domination operation, Deputy Inspector General of Police (South Bastar Range) Sundarraj P told PTI.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(sentence_list)\n",
    "summary = \" \".join(str(x) for x in sentence_list)\n",
    "print(summary)\n",
    "# save the data in another file, names sum.txt\n",
    "f = open('final3.txt', 'a+')\n",
    "#print(type(f))\n",
    "f.write('\\n')\n",
    "f.write(summary)\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An officer of the Central Reserve Police Force (CRPF) officer was killed in a gunfight with the Maoists in a dense forest in the insurgency-hit Sukma district of Chhattisgarh, police said on Saturday.\n",
      "The gunfight took place last night in Kistaram police station area when a team of CRPF's 212th battalion was out on a search-cum-area domination operation, Deputy Inspector General of Police (South Bastar Range) Sundarraj P told PTI.\n"
     ]
    }
   ],
   "source": [
    "for lines in sentence_list:\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please feel free to contribue for any improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
